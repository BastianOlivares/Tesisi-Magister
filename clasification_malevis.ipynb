{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn # neural network\n",
    "import torch.nn.functional as F # funcitions Relu\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms, models\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DIRECIONES\n",
    "direciones_img = {\n",
    "    \"alexnet\" : 'Img/Alexnet',\n",
    "    'vgg11' : 'Img/VGG/11',\n",
    "    'vgg16' : 'Img/VGG/16',\n",
    "    'resnet18' : 'Img/RESNET/18',\n",
    "    'resnet34' : 'Img/RESNET/32',\n",
    "    'resnet50' : 'Img/RESNET/50',\n",
    "    'resnet101' : 'Img/RESNET/101',\n",
    "    'inception' : 'Img/INCEPTION/V1',\n",
    "    'inception_v3' : 'Img/INCEPTION/V3',\n",
    "    'densenet121' : 'Img/DENSENET/121',\n",
    "    'densenet169' : 'Img/DENSENET/169',\n",
    "    'densenet201' : 'Img/DENSENET/201',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Number of GPUs available: 1\n",
      "GPU 0: NVIDIA GeForce RTX 3090\n"
     ]
    }
   ],
   "source": [
    "# Verificar si CUDA está disponible y qué dispositivo se está usando\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    # Contar el número de GPUs disponibles\n",
    "    num_gpus = torch.cuda.device_count()\n",
    "    print(f\"Number of GPUs available: {num_gpus}\")\n",
    "\n",
    "    # Mostrar el nombre de cada GPU\n",
    "    for i in range(num_gpus):\n",
    "        print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "else:\n",
    "        print(\"CUDA is not available. Using CPU.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x1c3041cc950>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# =============================\n",
    "# =======LECTURA DATASET=======\n",
    "# =============================\n",
    "transformaciones = transforms.Compose([\n",
    "    transforms.ToTensor()                                                               # Convierte las imágenes a tensores de PyTorch\n",
    "])\n",
    "\n",
    "# Carga las imágenes desde la carpeta --> TRAIN\n",
    "carpeta_imagenes_train = \"./Datasets/malevis/malevis_train_val_224x224/train\"\n",
    "dataset_train = datasets.ImageFolder(carpeta_imagenes_train, transform=transformaciones)\n",
    "dataloader_train = DataLoader(dataset_train, batch_size=32, shuffle=True)\n",
    "dataloader_train\n",
    "\n",
    "# Carga las imágenes desde la carpeta --> TEST\n",
    "carpeta_imagenes_test = \"./Datasets/malevis/malevis_train_val_224x224/val\"\n",
    "dataset_test = datasets.ImageFolder(carpeta_imagenes_test, transform=transformaciones)\n",
    "dataloader_test = DataLoader(dataset_test, batch_size=32, shuffle=True)\n",
    "dataloader_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\siner\\OneDrive\\Desktop\\Bastian\\env\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\siner\\OneDrive\\Desktop\\Bastian\\env\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "#=====================\n",
    "#======MODELOS========\n",
    "#=====================\n",
    "model = models.alexnet(pretrained=False)\n",
    "# Puedes cambiar el modelo según sea necesario:\n",
    "# model = models.vgg11(pretrained=False)\n",
    "# model = models.vgg16(pretrained=False)\n",
    "# model = models.resnet18(pretrained=False)\n",
    "# model = models.resnet34(pretrained=False)\n",
    "# model = models.resnet50(pretrained=False)\n",
    "# model = models.resnet101(pretrained=False)\n",
    "# model = models.inception_v3(pretrained=False)\n",
    "# model = models.densenet121(pretrained=False)\n",
    "# model = models.densenet169(pretrained=False)\n",
    "# model = models.densenet201(pretrained=False)\n",
    "\n",
    "# Enviar el modelo a la GPU (o a CPU si no hay GPU)\n",
    "model = model.to(device=device)\n",
    "\n",
    "# Configurar DataParallel si hay más de una GPU disponible\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(f\"Using {torch.cuda.device_count()} GPUs!\")\n",
    "    model = torch.nn.DataParallel(model)\n",
    "\n",
    "# Definir el directorio de imágenes\n",
    "img_dir = direciones_img['alexnet']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, criterion, optimizer, device, num_epochs=10):\n",
    "    train_losses = []\n",
    "    val_accuracies = []\n",
    "    \n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "        \n",
    "        epoch_loss = running_loss / len(train_loader.dataset)\n",
    "        train_losses.append(epoch_loss)\n",
    "        \n",
    "        # Evaluación de la precisión en el conjunto de validación\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        accuracy = correct / total\n",
    "        val_accuracies.append(accuracy)\n",
    "        # print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}, Accuracy: {accuracy:.4f}\")\n",
    "    \n",
    "    return train_losses, val_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses, val_accuracies = train_model( model, dataloader_train, dataloader_test, criterion, optimizer, device, num_epochs=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graficar la pérdida de entrenamiento\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(range(1, len(train_losses) + 1), train_losses, label='Training Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss Over Epochs')\n",
    "plt.legend()\n",
    "plt.savefig(img_dir + '/loss.jpeg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graficar la precisión de validación\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(range(1, len(val_accuracies) + 1), val_accuracies, label='Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Validation Accuracy Over Epochs')\n",
    "plt.legend()\n",
    "plt.savefig(img_dir + '/precision.jpeg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_precision_recall(model, dataloader, device):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "    \n",
    "    # Calcula precisión y recall\n",
    "    precision = precision_score(all_labels, all_predictions, average='weighted')\n",
    "    recall = recall_score(all_labels, all_predictions, average='weighted')\n",
    "    \n",
    "    # Calcula matriz de confusión\n",
    "    conf_matrix = confusion_matrix(all_labels, all_predictions)\n",
    "    \n",
    "    return precision, recall, conf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision, recall, conf_matrix = calculate_precision_recall(model, dataloader_test, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(conf_matrix, class_names):\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.xlabel('Predicted Labels')\n",
    "    plt.ylabel('True Labels')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.savefig(img_dir + '/matrix_confusion.jpeg')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = dataset_test.classes\n",
    "plot_confusion_matrix(conf_matrix, class_names)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
